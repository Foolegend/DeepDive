{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbGTWjbacs_H"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CRCTransformers/deepdive-book/blob/main/Chapter-2-Machine-Translation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NRm0-I-L5A_"
      },
      "source": [
        "# Machine Translation Comparison: Attention vs. Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNqyj0qsNm5J",
        "outputId": "824ffcb7-726b-42c5-a44f-43c2ff338b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'fr' are deprecated. Please use the\n",
            "full pipeline package name 'fr_core_news_sm' instead.\u001b[0m\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoKlMXVRe3PJ",
        "outputId": "ae69a750-b193-43f9-e6f2-575089881129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ecco\n",
            "  Downloading ecco-0.1.2-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: transformers~=4.2 in /usr/local/lib/python3.11/dist-packages (from ecco) (4.51.3)\n",
            "Requirement already satisfied: seaborn~=0.11 in /usr/local/lib/python3.11/dist-packages (from ecco) (0.13.2)\n",
            "INFO: pip is looking at multiple versions of ecco to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ecco-0.1.1-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "  Downloading ecco-0.1.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "  Downloading ecco-0.0.15-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "  Downloading ecco-0.0.14-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading ecco-0.0.13-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading ecco-0.0.12-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting transformers<3.5 (from ecco)\n",
            "  Downloading transformers-3.4.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting ecco\n",
            "  Downloading ecco-0.0.10-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: pip is still looking at multiple versions of ecco to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ecco-0.0.9-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading ecco-0.0.8-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading ecco-0.0.7-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ecco-0.0.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ecco-0.0.5-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading ecco-0.0.4-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading ecco-0.0.3-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting transformers~=3.1 (from ecco)\n",
            "  Downloading transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting ecco\n",
            "  Downloading ecco-0.0.2-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn~=0.11->ecco) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn~=0.11->ecco) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn~=0.11->ecco) (3.10.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers~=3.1 (from ecco)\n",
            "  Downloading transformers-3.5.0-py3-none-any.whl.metadata (32 kB)\n",
            "  Downloading transformers-3.3.1-py3-none-any.whl.metadata (29 kB)\n",
            "  Downloading transformers-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "  Downloading transformers-3.2.0-py3-none-any.whl.metadata (28 kB)\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ecco\n",
            "  Downloading ecco-0.0.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading ecco-0.0.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading ecco-0.0.0-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ecco\n",
            "Successfully installed ecco-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ecco --only-binary=:all:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbL_7Jptdcb_",
        "outputId": "16c14920-b7cf-4d44-a96c-503d84431eb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QM7AloCeRSm",
        "outputId": "21fc1cf2-fee1-41ef-a27b-3293de763c0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "id": "ugDuE3NLe6_v",
        "outputId": "8391f2cd-ec9d-4ba8-e760-ff3a73c65a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.3.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchtext, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KxOvYmiaqBGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a0f1f0-a445-428b-e193-d00146176b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker\n",
        "import ecco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9AwBvnPLp2U8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://download.pytorch.org/tutorial/data.zip && unzip data.zip && rm data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EipCmDVDqONN"
      },
      "outputs": [],
      "source": [
        "MAX_SENTENCE_LENGTH = 20\n",
        "FILTER_TO_BASIC_PREFIXES = False\n",
        "SAVE_DIR = os.path.join(\".\", \"models\")\n",
        "\n",
        "ENCODER_EMBEDDING_DIM = 256\n",
        "ENCODER_HIDDEN_SIZE = 256\n",
        "DECODER_EMBEDDING_DIM = 256\n",
        "DECODER_HIDDEN_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jii6i_WgqhAN",
        "outputId": "0df340de-48ca-43fc-e739-67a8f2649eaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMmAsrJpqiPU",
        "outputId": "c50db619-1471-41a8-d217-302c69c163b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  4 17:02:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XntXtjJDq_nd",
        "outputId": "5bdf779f-1fef-4ee0-8a8e-fda71fe04336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135,842 English-French phrase pairs.\n",
            "\n",
            "~~~~~ Examples: ~~~~~\n",
            "English:  Will you be going?\n",
            "French:   Iras-tu ?\n",
            "\n",
            "English:  She is old.\n",
            "French:   Elle est vieille.\n",
            "\n",
            "English:  She has about 2,000 books.\n",
            "French:   Elle a à peu près 2000 livres.\n",
            "\n",
            "English:  Please take me to this address.\n",
            "French:   Amenez-moi à cette adresse.\n",
            "\n",
            "English:  Could you put this bag somewhere else?\n",
            "French:   Pourrais-tu mettre ce sac ailleurs ?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('data/eng-fra.txt', encoding='utf-8') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "print(f\"{len(lines):,} English-French phrase pairs.\\n\")\n",
        "print(\"~~~~~ Examples: ~~~~~\")\n",
        "for example in random.choices(lines, k=5):\n",
        "    pair = example.split('\\t')\n",
        "    print(f\"English:  {pair[0]}\")\n",
        "    print(f\"French:   {pair[1]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ndW_mwoxrCbd"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", \" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "00SrYNQ1rE3t"
      },
      "outputs": [],
      "source": [
        "def filterPair(p, max_length, prefixes):\n",
        "    good_length = (len(p[0].split(' ')) < max_length) and (len(p[1].split(' ')) < max_length)\n",
        "    if len(prefixes) == 0:\n",
        "        return good_length\n",
        "    else:\n",
        "        return good_length and p[0].startswith(prefixes)\n",
        "\n",
        "def filterPairs(pairs, max_length, prefixes=()):\n",
        "    return [pair for pair in pairs if filterPair(pair, max_length, prefixes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9b2fcr43rHJN"
      },
      "outputs": [],
      "source": [
        "def prepareData(lines, filter=False, reverse=False, max_length=10, prefixes=()):\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    print(f\"Given {len(pairs):,} sentence pairs.\")\n",
        "\n",
        "    if filter:\n",
        "        pairs = filterPairs(pairs, max_length=max_length, prefixes=prefixes)\n",
        "        print(f\"After filtering, {len(pairs):,} remain.\")\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AiQ4nT9FrIgl"
      },
      "outputs": [],
      "source": [
        "basic_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \",\n",
        "    'are you', 'am i ',\n",
        "    'were you', 'was i ',\n",
        "    'where are', 'where is',\n",
        "    'what is', 'what are'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyZc4Y4VrKGF",
        "outputId": "dc106dc1-5077-433c-fdb6-dae1e09d2a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 135,842 sentence pairs.\n",
            "After filtering, 135,284 remain.\n"
          ]
        }
      ],
      "source": [
        "pairs = prepareData(lines,\n",
        "                    filter=True,\n",
        "                    max_length=MAX_SENTENCE_LENGTH,\n",
        "                    prefixes=basic_prefixes if FILTER_TO_BASIC_PREFIXES else ())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGktCWkDMM6j"
      },
      "source": [
        "## Prepare Data for Modeling\n",
        "We now have to tokenize the text pairs to create numeric inputs for our embedding layers. We'll use tokenizers from `spaCy` accessed via the `torchtext` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BkXaviYerLW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50c048b-7e38-4615-9842-c2c13362205a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"fr\" could not be loaded, trying \"fr_core_news_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "fr_tokenizer = get_tokenizer('spacy', language='fr')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aLqKlMzFMWH4"
      },
      "outputs": [],
      "source": [
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTDrlX_tUZdW"
      },
      "outputs": [],
      "source": [
        "en_list = []\n",
        "fr_list = []\n",
        "en_counter = Counter()\n",
        "fr_counter = Counter()\n",
        "en_lengths = []\n",
        "fr_lengths = []\n",
        "for en, fr in pairs:\n",
        "    en_toks = en_tokenizer(en)\n",
        "    fr_toks = fr_tokenizer(fr)\n",
        "    en_list += [en_toks]\n",
        "    fr_list += [fr_toks]\n",
        "    en_counter.update(en_toks)\n",
        "    fr_counter.update(fr_toks)\n",
        "    en_lengths.append(len(en_toks))\n",
        "    fr_lengths.append(len(fr_toks))\n",
        "\n",
        "en_vocab = build_vocab_from_iterator(en_list, specials=SPECIALS)\n",
        "fr_vocab = build_vocab_from_iterator(fr_list, specials=SPECIALS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSD1gXHoNZxW"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax0 = fig.add_subplot(211)\n",
        "ax0.hist(en_lengths, rwidth=0.8, color='gray')\n",
        "ax0.set_title(\"English Sentence Length\")\n",
        "ax0.set_xlabel(\"# Tokens in Sentence\")\n",
        "\n",
        "ax1 = fig.add_subplot(212)\n",
        "ax1.hist(fr_lengths, rwidth=0.8, color='gray')\n",
        "ax1.set_title(\"French Sentence Length\")\n",
        "ax1.set_xlabel(\"# Tokens in Sentence\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvi5sR5WUkyv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.hist2d(en_lengths, fr_lengths, bins=MAX_SENTENCE_LENGTH-2, cmap='binary')\n",
        "plt.title(\"Joint Distribution of Sentence Lengths\", fontsize=14)\n",
        "plt.xlabel(\"# English Tokens\")\n",
        "plt.ylabel(\"# French Tokens\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48sjYra6VlYZ"
      },
      "source": [
        "Most common words in the vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDMTTzMzVnJP"
      },
      "outputs": [],
      "source": [
        "def plot_top_words(counter, k=20, ax=None):\n",
        "    top_k = counter.most_common(k)\n",
        "    words, freqs = zip(*reversed(top_k))\n",
        "\n",
        "    if ax is None:\n",
        "        plt.barh(words, freqs, color='gray')\n",
        "    else:\n",
        "        ax.barh(words, freqs, color='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-LhGXQxVrFI"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "ax0 = fig.add_subplot(121)\n",
        "plot_top_words(en_counter, ax=ax0)\n",
        "ax0.set_title(\"Top 20 English Words\")\n",
        "ax0.set_xlabel(\"Raw Frequency\")\n",
        "\n",
        "ax1 = fig.add_subplot(122)\n",
        "plot_top_words(fr_counter, ax=ax1)\n",
        "ax1.set_title(\"Top 20 French Words\")\n",
        "ax1.set_xlabel(\"Raw Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKr8V15uVxuu"
      },
      "outputs": [],
      "source": [
        "VALID_PCT = 0.1\n",
        "TEST_PCT  = 0.1\n",
        "\n",
        "train_data = []\n",
        "valid_data = []\n",
        "test_data = []\n",
        "\n",
        "random.seed(6547)\n",
        "for (en, fr) in pairs:\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(en)])\n",
        "    fr_tensor_ = torch.tensor([fr_vocab[token] for token in fr_tokenizer(fr)])\n",
        "    random_draw = random.random()\n",
        "    if random_draw <= VALID_PCT:\n",
        "        valid_data.append((en_tensor_, fr_tensor_))\n",
        "    elif random_draw <= VALID_PCT + TEST_PCT:\n",
        "        test_data.append((en_tensor_, fr_tensor_))\n",
        "    else:\n",
        "        train_data.append((en_tensor_, fr_tensor_))\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "  Training pairs: {len(train_data):,}\n",
        "Validation pairs: {len(valid_data):,}\n",
        "      Test pairs: {len(test_data):,}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWGvhpE6XQJR"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = en_vocab['<pad>']\n",
        "BOS_IDX = en_vocab['<bos>']\n",
        "EOS_IDX = en_vocab['<eos>']\n",
        "\n",
        "for en_id, fr_id in zip(en_vocab.lookup_indices(SPECIALS), fr_vocab.lookup_indices(SPECIALS)):\n",
        "  assert en_id == fr_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtVcafTdXSoQ"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch):\n",
        "    '''\n",
        "    Prepare English and French examples for batch-friendly modeling by appending\n",
        "    BOS/EOS tokens to each, stacking the tensors, and filling trailing spaces of\n",
        "    shorter sentences with the <pad> token. To be used as the collate_fn in the\n",
        "    English-to-French DataLoader.\n",
        "\n",
        "    Input:\n",
        "    - data_batch, an iterable of (English, French) tuples from the datasets\n",
        "      created above\n",
        "\n",
        "    Outputs\n",
        "    - en_batch: a (max length X batch size) tensor of English token IDs\n",
        "    - fr_batch: a (max length X batch size) tensor of French token IDs\n",
        "    '''\n",
        "    en_batch, fr_batch = [], []\n",
        "    for (en_item, fr_item) in data_batch:\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcoFsPpvXzan"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjucwYbX07V"
      },
      "outputs": [],
      "source": [
        "for i, (en_id, fr_id) in enumerate(train_iter):\n",
        "    print('English:', ' '.join([en_vocab.lookup_token(idx) for idx in en_id[:, 0]]))\n",
        "    print('French:', ' '.join([fr_vocab.lookup_token(idx) for idx in fr_id[:, 0]]))\n",
        "    if i == 4:\n",
        "        break\n",
        "    else:\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Ff9InLYCIJ"
      },
      "source": [
        "## LSTM with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwloDNAmYVv8"
      },
      "source": [
        "### Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P1DZqsmX2FA"
      },
      "outputs": [],
      "source": [
        "class BahdanauEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, encoder_hidden_dim,\n",
        "                 decoder_hidden_dim, dropout_p):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_hidden_dim = encoder_hidden_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
        "        self.linear = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Encode a source sentence.\n",
        "\n",
        "        Input:\n",
        "          - x: a (sequence length, batch size) tensor of token IDs in source language\n",
        "\n",
        "        Output:\n",
        "          - outputs: encoder outputs at each time step, given as a tensor of size\n",
        "            (sequence length, batch size, encoder hidden dim * 2)\n",
        "          - hidden: final hidden state from RNN, with directions concatenated and\n",
        "            fed through linear layer; tensor of size (batch size, decoder hidden dim)\n",
        "        '''\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.linear(\n",
        "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        ))\n",
        "\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL5HiA7zYEeg"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttentionQKV(nn.Module):\n",
        "    def __init__(self, hidden_size, query_size=None, key_size=None, dropout_p=0.15):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        # assume bidirectional encoder, but can specify otherwise\n",
        "        self.key_size = 2*hidden_size if key_size is None else key_size\n",
        "\n",
        "        self.query_layer = nn.Linear(self.query_size, hidden_size)\n",
        "        self.key_layer = nn.Linear(self.key_size, hidden_size)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, src_mask=None):\n",
        "        '''\n",
        "        Calculate attention weights using query and key features, with\n",
        "        an optional mask for input sequences.\n",
        "\n",
        "        Inputs:\n",
        "          - hidden: most recent RNN hidden state; (B, Dec)\n",
        "          - encoder_outputs: RNN outputs at individual time steps with\n",
        "            directions concatenated; (L, B, 2*Enc)\n",
        "          - src_mask: boolean tensor of same size as source tokens (Src, B)\n",
        "            where False denotes tokens to be ignored\n",
        "\n",
        "        Outputs:\n",
        "          - attention weights: (B, src) tensor of softmax attention\n",
        "            weights to be applied to downstream values\n",
        "        '''\n",
        "\n",
        "        # (B, H)\n",
        "        query_out = self.query_layer(hidden)\n",
        "\n",
        "        # (Src, B, 2*H) --> (Src, B, H)\n",
        "        key_out = self.key_layer(encoder_outputs)\n",
        "\n",
        "        # (B, H) + (Src, B, H) = (Src, B, H)\n",
        "        energy_input = torch.tanh(query_out + key_out)\n",
        "\n",
        "        # (Src, B, H) --> (Src, B, 1) --> (Src, B)\n",
        "        energies = self.energy_layer(energy_input).squeeze(2)\n",
        "\n",
        "        # if a mask is provided, remove masked tokens from softmax calc\n",
        "        if src_mask is not None:\n",
        "            energies.data.masked_fill_(src_mask == 0, float(\"-inf\"))\n",
        "\n",
        "        # softmax over the length dimension\n",
        "        weights = F.softmax(energies, dim=0)\n",
        "\n",
        "        # return as (B, Src) as expected by later multiplication\n",
        "        return weights.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESzxttziYIQT"
      },
      "outputs": [],
      "source": [
        "class BahdanauDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, encoder_hidden_dim,\n",
        "                 decoder_hidden_dim, attention, dropout_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder_hidden_dim = encoder_hidden_dim\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.attention = attention # allowing for custom attention\n",
        "        self.gru = nn.GRU((encoder_hidden_dim * 2) + embedding_dim,\n",
        "                          decoder_hidden_dim)\n",
        "        self.out = nn.Linear((encoder_hidden_dim * 2) + embedding_dim + decoder_hidden_dim,\n",
        "                             output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, src_mask=None):\n",
        "        '''\n",
        "        Decode an encoder's output.\n",
        "\n",
        "        B: batch size\n",
        "        S: source sentence length\n",
        "        T: target sentence length\n",
        "        O: output size (target vocab size)\n",
        "        Enc: encoder hidden dim\n",
        "        Dec: decoder hidden dim\n",
        "        Emb: embedding dim\n",
        "\n",
        "        Inputs:\n",
        "          - input: a vector of length B giving the most recent decoded token\n",
        "          - hidden: a (B, Dec) most recent RNN hidden state\n",
        "          - encoder_outputs: (S, B, 2*Enc) sequence of outputs from encoder RNN\n",
        "\n",
        "        Outputs:\n",
        "          - output: logits for next token in the sequence (B, O)\n",
        "          - hidden: a new (B, Dec) RNN hidden state\n",
        "          - attentions: (B, S) attention weights for the current token over the source sentence\n",
        "        '''\n",
        "\n",
        "        # (B) --> (1, B)\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        attentions = self.attention(hidden, encoder_outputs, src_mask)\n",
        "\n",
        "        # (B, S) --> (B, 1, S)\n",
        "        a = attentions.unsqueeze(1)\n",
        "\n",
        "        # (S, B, 2*Enc) --> (B, S, 2*Enc)\n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
        "\n",
        "        # weighted encoder representation\n",
        "        # (B, 1, S) @ (B, S, 2*Enc) = (B, 1, 2*Enc)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        # (B, 1, 2*Enc) --> (1, B, 2*Enc)\n",
        "        weighted = weighted.transpose(0, 1)\n",
        "\n",
        "        # concat (1, B, Emb) and (1, B, 2*Enc)\n",
        "        # results in (1, B, Emb + 2*Enc)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        assert (output == hidden).all()\n",
        "\n",
        "        # get rid of empty leading dimensions\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        # concatenate the pieces above\n",
        "        # (B, Dec), (B, 2*Enc), and (B, Emb)\n",
        "        # result is (B, Dec + 2*Enc + Emb)\n",
        "        linear_input = torch.cat((output, weighted, embedded), dim=1)\n",
        "\n",
        "        # (B, Dec + 2*Enc + Emb) --> (B, O)\n",
        "        output = self.out(linear_input)\n",
        "\n",
        "        return output, hidden.squeeze(0), attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEe-xIgrYKcg"
      },
      "outputs": [],
      "source": [
        "class BahdanauSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder.to(device)\n",
        "        self.decoder = decoder.to(device)\n",
        "        self.device = device\n",
        "        self.tgt_vocab_size = decoder.output_dim\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, teacher_forcing_ratio=0.5, return_attentions=False):\n",
        "\n",
        "        tgt_length, batch_size = tgt.shape\n",
        "\n",
        "        # store decoder outputs\n",
        "        outputs = torch.zeros(tgt_length, batch_size, self.tgt_vocab_size).to(self.device)\n",
        "        # attentions = torch.zeros(tgt_length, batch_size, )\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        hidden = hidden.squeeze(1) # B, 1, Enc --> B, Enc (if necessary)\n",
        "\n",
        "        # start with <bos> as the decoder input\n",
        "        decoder_input = tgt[0, :]\n",
        "        attentions = []\n",
        "\n",
        "        for t in range(1, tgt_length):\n",
        "            decoder_output, hidden, attention = self.decoder(decoder_input, hidden, encoder_outputs, src_mask)\n",
        "            outputs[t] = decoder_output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top_token = decoder_output.max(1)[1]\n",
        "            decoder_input = (tgt[t] if teacher_force else top_token)\n",
        "            attentions.append(attention.unsqueeze(-1))\n",
        "\n",
        "        if return_attentions:\n",
        "            return outputs, torch.cat(attentions, dim=-1)\n",
        "        else:\n",
        "            return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgsWpyALYXq0"
      },
      "source": [
        "### Train Bahdanau seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOIpKlPBYMTY"
      },
      "outputs": [],
      "source": [
        "class MultipleOptimizer(object):\n",
        "    def __init__(self, *op):\n",
        "        self.optimizers = op\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for op in self.optimizers:\n",
        "            op.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        for op in self.optimizers:\n",
        "            op.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M81Of8O1Yam4"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, loss_fn, device, clip=None):\n",
        "    model.train()\n",
        "    if model.device != device:\n",
        "        model = model.to(device)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with tqdm(total=len(iterator), leave=False) as t:\n",
        "        for i, (src, tgt) in enumerate(iterator):\n",
        "            src_mask = (src != PAD_IDX).to(device)\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(src, tgt, src_mask)\n",
        "\n",
        "            loss = loss_fn(output[1:].view(-1, output.shape[2]),\n",
        "                           tgt[1:].view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / (i+1)\n",
        "            t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
        "                          ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
        "            t.update()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4F6oC_WYcUB"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, loss_fn, device):\n",
        "    model.eval()\n",
        "    if model.device != device:\n",
        "        model = model.to(device)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(iterator), leave=False) as t:\n",
        "            for i, (src, tgt) in enumerate(iterator):\n",
        "                src_mask = (src != PAD_IDX).to(device)\n",
        "                src = src.to(device)\n",
        "                tgt = tgt.to(device)\n",
        "\n",
        "                output = model(src, tgt, src_mask, teacher_forcing_ratio=0)\n",
        "                loss = loss_fn(output[1:].view(-1, output.shape[2]),\n",
        "                               tgt[1:].view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                avg_loss = epoch_loss / (i+1)\n",
        "                t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
        "                              ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
        "                t.update()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuAxOcA_Ydsg"
      },
      "outputs": [],
      "source": [
        "enc = BahdanauEncoder(input_dim=len(en_vocab),\n",
        "                      embedding_dim=ENCODER_EMBEDDING_DIM,\n",
        "                      encoder_hidden_dim=ENCODER_HIDDEN_SIZE,\n",
        "                      decoder_hidden_dim=DECODER_HIDDEN_SIZE,\n",
        "                      dropout_p=0.15)\n",
        "\n",
        "attn = BahdanauAttentionQKV(DECODER_HIDDEN_SIZE)\n",
        "\n",
        "dec = BahdanauDecoder(output_dim=len(fr_vocab),\n",
        "                      embedding_dim=DECODER_EMBEDDING_DIM,\n",
        "                      encoder_hidden_dim=ENCODER_HIDDEN_SIZE,\n",
        "                      decoder_hidden_dim=DECODER_HIDDEN_SIZE,\n",
        "                      attention=attn,\n",
        "                      dropout_p=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrYSqXa-YfCY"
      },
      "outputs": [],
      "source": [
        "seq2seq = BahdanauSeq2Seq(enc, dec, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr_K0jEBYgXA"
      },
      "outputs": [],
      "source": [
        "def count_params(model, return_int=False):\n",
        "    params = sum([torch.prod(torch.tensor(x.shape)).item() for x in model.parameters() if x.requires_grad])\n",
        "    if return_int:\n",
        "        return params\n",
        "    else:\n",
        "        print(\"There are {:,} trainable parameters in this model.\".format(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8OxApzwYhyx"
      },
      "outputs": [],
      "source": [
        "count_params(seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXPxsg-bYjSJ"
      },
      "outputs": [],
      "source": [
        "enc_optim = torch.optim.AdamW(seq2seq.encoder.parameters(), lr=1e-4)\n",
        "dec_optim = torch.optim.AdamW(seq2seq.decoder.parameters(), lr=1e-4)\n",
        "optims = MultipleOptimizer(enc_optim, dec_optim)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIO3Ml0qYlNI"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 10 # clipping value, or None to prevent gradient clipping\n",
        "EARLY_STOPPING_EPOCHS = 2\n",
        "\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    print(f\"Creating directory {SAVE_DIR}\")\n",
        "    os.mkdir(SAVE_DIR)\n",
        "\n",
        "model_path = os.path.join(SAVE_DIR, 'bahdanau_en_fr.pt')\n",
        "bahdanau_metrics = {}\n",
        "best_valid_loss = float(\"inf\")\n",
        "early_stopping_count = 0\n",
        "for epoch in tqdm(range(N_EPOCHS), leave=False, desc=\"Epoch\"):\n",
        "    train_loss = train(seq2seq, train_iter, optims, loss_fn, device, clip=CLIP)\n",
        "    valid_loss = evaluate(seq2seq, valid_iter, loss_fn, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        tqdm.write(f\"Checkpointing at epoch {epoch + 1}\")\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(seq2seq.state_dict(), model_path)\n",
        "        early_stopping_count = 0\n",
        "    else:\n",
        "        early_stopping_count += 1\n",
        "\n",
        "    bahdanau_metrics[epoch+1] = dict(\n",
        "        train_loss = train_loss,\n",
        "        train_ppl = np.exp(train_loss),\n",
        "        valid_loss = valid_loss,\n",
        "        valid_ppl = np.exp(valid_loss)\n",
        "    )\n",
        "\n",
        "    if early_stopping_count == EARLY_STOPPING_EPOCHS:\n",
        "        tqdm.write(f\"Early stopping triggered in epoch {epoch + 1}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjG310nWYntA"
      },
      "outputs": [],
      "source": [
        "seq2seq.load_state_dict(torch.load(model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nW_GeopRZnv"
      },
      "outputs": [],
      "source": [
        "bahdanau_metrics_df = pd.DataFrame(bahdanau_metrics).T\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(bahdanau_metrics_df['train_loss'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
        "plt.plot(bahdanau_metrics_df['valid_loss'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
        "plt.legend()\n",
        "plt.title(\"Bahdanau Attention: Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(bahdanau_metrics_df['train_ppl'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
        "plt.plot(bahdanau_metrics_df['valid_ppl'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
        "plt.legend()\n",
        "plt.title(\"Bahdanau Attention: Perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eCaspjPfeUJ"
      },
      "outputs": [],
      "source": [
        "best_valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wANKaMO7fkXy"
      },
      "outputs": [],
      "source": [
        "def predict_text(model, text, device=device, src_vocab=en_vocab, src_tokenizer=en_tokenizer, tgt_vocab=fr_vocab):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = [src_vocab[token] for token in src_tokenizer(text)]\n",
        "        input_ids = [BOS_IDX] + input_ids + [EOS_IDX]\n",
        "        input_tensor = torch.tensor(input_ids).to(device).unsqueeze(1) # add fake batch dim\n",
        "        max_len = 2*len(input_ids)\n",
        "        encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "\n",
        "        output = torch.tensor([BOS_IDX]).to(device)\n",
        "\n",
        "        decoder_outputs = torch.zeros(max_len, 1, len(tgt_vocab)).to(device)\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_len, len(input_ids))\n",
        "        for t in range(0, max_len):\n",
        "            output, hidden, attn = model.decoder(output, hidden, encoder_outputs)\n",
        "            decoder_attentions[t] = attn.data\n",
        "            decoder_outputs[t] = output\n",
        "            output = output.argmax(1)\n",
        "\n",
        "            if output.item() == EOS_IDX:\n",
        "                decoded_words.append('<eos>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(tgt_vocab.lookup_token(output.item()))\n",
        "\n",
        "        output_sentence = ' '.join(decoded_words)\n",
        "        return output_sentence, decoder_attentions[:(t+1)]\n",
        "\n",
        "def show_attention(input_sentence, output_sentence, attentions, figsize=(8,6)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='gray')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    src = ['', '<bos>'] + input_sentence.split(' ') + ['<eos>']\n",
        "    tgt = [''] + output_sentence.split(' ')\n",
        "    ax.set_xticklabels(src, rotation=90)\n",
        "    ax.set_yticklabels(tgt)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def predict_and_show_attention(sentence, model, device):\n",
        "    result, attentions = predict_text(model, sentence, device)\n",
        "\n",
        "    print(\"Input  >>>\", sentence)\n",
        "    print(\"Output >>>\", result)\n",
        "\n",
        "    show_attention(sentence, result, attentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8NBIEwnfqiy"
      },
      "outputs": [],
      "source": [
        "predict_and_show_attention(\"i am going to the store\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V_qLSmjfry6"
      },
      "outputs": [],
      "source": [
        "predict_and_show_attention(\"how long will you be here ?\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3jIGvhypLYT"
      },
      "outputs": [],
      "source": [
        "predict_and_show_attention(\"can we please go to the library ?\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y53h3N7ApPck"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_show_attention(\"her family moved away last year .\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ft1azQxpRBL"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_show_attention(\"two thousand people fit into this hall .\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmQ6uC3xpX0z"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_show_attention(\"the poor young man finally became a great artist .\", seq2seq, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvh8EAEGpbqS"
      },
      "source": [
        "## Transformer Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ImZzaoMpZ5s"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout_p=0.1, max_len=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, num_attention_heads,\n",
        "                 num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
        "                 max_seq_length, pos_dropout, transformer_dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed_src = nn.Embedding(input_dim, d_model)\n",
        "        self.embed_tgt = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model, num_attention_heads, num_encoder_layers,\n",
        "                                          num_decoder_layers, dim_feedforward, transformer_dropout)\n",
        "        self.output = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                src=None,\n",
        "                tgt=None,\n",
        "                src_mask=None,\n",
        "                tgt_mask=None,\n",
        "                src_key_padding_mask=None,\n",
        "                tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None,\n",
        "                src_embeds=None,\n",
        "                tgt_embeds=None):\n",
        "\n",
        "        if (src_embeds is None) and (src is not None):\n",
        "            if (tgt_embeds is None) and (tgt is not None):\n",
        "                src_embeds, tgt_embeds = self._embed_tokens(src, tgt)\n",
        "        elif (src_embeds is not None) and (src is not None):\n",
        "            raise ValueError(\"Must specify exactly one of src and src_embeds\")\n",
        "        elif (src_embeds is None) and (src is None):\n",
        "            raise ValueError(\"Must specify exactly one of src and src_embeds\")\n",
        "        elif (tgt_embeds is not None) and (tgt is not None):\n",
        "            raise ValueError(\"Must specify exactly one of tgt and tgt_embeds\")\n",
        "        elif (tgt_embeds is None) and (tgt is None):\n",
        "            raise ValueError(\"Must specify exactly one of tgt and tgt_embeds\")\n",
        "\n",
        "        output = self.transformer(src_embeds,\n",
        "                                  tgt_embeds,\n",
        "                                  tgt_mask=tgt_mask,\n",
        "                                  src_key_padding_mask=src_key_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        return self.output(output)\n",
        "\n",
        "    def _embed_tokens(self, src, tgt):\n",
        "        src_embeds = self.embed_src(src) * np.sqrt(self.d_model)\n",
        "        tgt_embeds = self.embed_tgt(tgt) * np.sqrt(self.d_model)\n",
        "\n",
        "        src_embeds = self.pos_enc(src_embeds)\n",
        "        tgt_embeds = self.pos_enc(tgt_embeds)\n",
        "        return src_embeds, tgt_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B44Ox5npfZc"
      },
      "outputs": [],
      "source": [
        "def train_transformer(model, iterator, optimizer, loss_fn, device, clip=None):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with tqdm(total=len(iterator), leave=False) as t:\n",
        "        for i, (src, tgt) in enumerate(iterator):\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            # Create tgt_inp and tgt_out (which is tgt_inp but shifted by 1)\n",
        "            tgt_inp, tgt_out = tgt[:-1, :], tgt[1:, :]\n",
        "\n",
        "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_inp.size(0)).to(device)\n",
        "            src_key_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "            tgt_key_padding_mask = (tgt_inp == PAD_IDX).transpose(0, 1)\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(src=src, tgt=tgt_inp,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           src_key_padding_mask = src_key_padding_mask,\n",
        "                           tgt_key_padding_mask = tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask = memory_key_padding_mask)\n",
        "\n",
        "            loss = loss_fn(output.view(-1, output.shape[2]),\n",
        "                           tgt_out.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / (i+1)\n",
        "            t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
        "                          ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
        "            t.update()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate_transformer(model, iterator, loss_fn, device):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(iterator), leave=False) as t:\n",
        "            for i, (src, tgt) in enumerate(iterator):\n",
        "                src = src.to(device)\n",
        "                tgt = tgt.to(device)\n",
        "\n",
        "                # Create tgt_inp and tgt_out (which is tgt_inp but shifted by 1)\n",
        "                tgt_inp, tgt_out = tgt[:-1, :], tgt[1:, :]\n",
        "\n",
        "                tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_inp.size(0)).to(device)\n",
        "                src_key_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "                tgt_key_padding_mask = (tgt_inp == PAD_IDX).transpose(0, 1)\n",
        "                memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "\n",
        "                output = model(src=src, tgt=tgt_inp,\n",
        "                               tgt_mask=tgt_mask,\n",
        "                               src_key_padding_mask = src_key_padding_mask,\n",
        "                               tgt_key_padding_mask = tgt_key_padding_mask,\n",
        "                               memory_key_padding_mask = memory_key_padding_mask)\n",
        "\n",
        "                loss = loss_fn(output.view(-1, output.shape[2]),\n",
        "                               tgt_out.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                avg_loss = epoch_loss / (i+1)\n",
        "                t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
        "                              ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
        "                t.update()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW6JG4n4pi_r"
      },
      "outputs": [],
      "source": [
        "transformer = TransformerModel(input_dim=len(en_vocab),\n",
        "                             output_dim=len(fr_vocab),\n",
        "                             d_model=256,\n",
        "                             num_attention_heads=8,\n",
        "                             num_encoder_layers=6,\n",
        "                             num_decoder_layers=6,\n",
        "                             dim_feedforward=2048,\n",
        "                             max_seq_length=32,\n",
        "                             pos_dropout=0.15,\n",
        "                             transformer_dropout=0.3)\n",
        "\n",
        "transformer = transformer.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY9VLrncpkOz"
      },
      "outputs": [],
      "source": [
        "count_params(transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1ZO3hLCplNs"
      },
      "outputs": [],
      "source": [
        "xf_optim = torch.optim.AdamW(transformer.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h7W2CDxpmnL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "N_EPOCHS = 50\n",
        "CLIP = 15 # clipping value, or None to prevent gradient clipping\n",
        "EARLY_STOPPING_EPOCHS = 5\n",
        "\n",
        "model_path = os.path.join(SAVE_DIR, 'transformer_en_fr.pt')\n",
        "transformer_metrics = {}\n",
        "best_valid_loss = float(\"inf\")\n",
        "early_stopping_count = 0\n",
        "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
        "    train_loss = train_transformer(transformer, train_iter, xf_optim, loss_fn, device, clip=CLIP)\n",
        "    valid_loss = evaluate_transformer(transformer, valid_iter, loss_fn, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        tqdm.write(f\"Checkpointing at epoch {epoch + 1}\")\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(transformer.state_dict(), model_path)\n",
        "        early_stopping_count = 0\n",
        "    elif epoch > EARLY_STOPPING_EPOCHS:\n",
        "        early_stopping_count += 1\n",
        "\n",
        "    transformer_metrics[epoch+1] = dict(\n",
        "        train_loss = train_loss,\n",
        "        train_ppl = np.exp(train_loss),\n",
        "        valid_loss = valid_loss,\n",
        "        valid_ppl = np.exp(valid_loss)\n",
        "    )\n",
        "\n",
        "    if early_stopping_count == EARLY_STOPPING_EPOCHS:\n",
        "        tqdm.write(f\"Early stopping triggered in epoch {epoch + 1}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeL99W2Epodc"
      },
      "outputs": [],
      "source": [
        "transformer.load_state_dict(torch.load(model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G9uu8WZSdiK"
      },
      "outputs": [],
      "source": [
        "transformer_metrics_df = pd.DataFrame(transformer_metrics).T\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(transformer_metrics_df['train_loss'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
        "plt.plot(transformer_metrics_df['valid_loss'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
        "plt.legend()\n",
        "plt.title(\"Transformer: Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(transformer_metrics_df['train_ppl'], label=\"Training\", color='gray', linestyle='solid', lw=2.5)\n",
        "plt.plot(transformer_metrics_df['valid_ppl'], label=\"Validation\", color='gray', linestyle='dashed', lw=2.5)\n",
        "plt.legend()\n",
        "plt.title(\"Transformer: Perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2m7IPRVpxKf"
      },
      "outputs": [],
      "source": [
        "best_valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-7E0QcTpxjk"
      },
      "outputs": [],
      "source": [
        "def predict_transformer(text, model,\n",
        "                        src_vocab=en_vocab,\n",
        "                        src_tokenizer=en_tokenizer,\n",
        "                        tgt_vocab=fr_vocab,\n",
        "                        device=device):\n",
        "\n",
        "    input_ids = [src_vocab[token] for token in src_tokenizer(text)]\n",
        "    input_ids = [BOS_IDX] + input_ids + [EOS_IDX]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(input_ids).to(device).unsqueeze(1) # add fake batch dim\n",
        "\n",
        "        causal_out = torch.ones(MAX_SENTENCE_LENGTH, 1).long().to(device) * BOS_IDX\n",
        "        for t in range(1, MAX_SENTENCE_LENGTH):\n",
        "            decoder_output = transformer(input_tensor, causal_out[:t, :])[-1, :, :]\n",
        "            next_token = decoder_output.data.topk(1)[1].squeeze()\n",
        "            causal_out[t, :] = next_token\n",
        "            if next_token.item() == EOS_IDX:\n",
        "                break\n",
        "\n",
        "        pred_words = [tgt_vocab.lookup_token(tok.item()) for tok in causal_out.squeeze(1)[1:(t)]]\n",
        "        return \" \".join(pred_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5QrKLZGp1P5"
      },
      "outputs": [],
      "source": [
        "predict_transformer(\"she is not my mother .\", transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk1BmtMcp2qP"
      },
      "outputs": [],
      "source": [
        "predict_transformer(\"i would go anywhere with you .\", transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAzKmQ0nfDsT"
      },
      "outputs": [],
      "source": [
        "transformer = transformer.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m4BxlLge9gb"
      },
      "outputs": [],
      "source": [
        "def _one_hot(token_ids, vocab_size, device=device):\n",
        "    return torch.zeros(token_ids.size(0), vocab_size).to(device).scatter_(1, token_ids, 1.)\n",
        "\n",
        "def get_embeds(embedding_matrix, position_embedding_layer, input_ids, device=device):\n",
        "    vocab_size = embedding_matrix.size(0)\n",
        "    one_hot_tensor = _one_hot(input_ids, vocab_size, device)\n",
        "\n",
        "    token_ids_tensor_one_hot = one_hot_tensor.to(device).clone().requires_grad_(True)\n",
        "    inputs_embeds = torch.matmul(token_ids_tensor_one_hot, embedding_matrix)\n",
        "    inputs_embeds = position_embedding_layer(inputs_embeds.unsqueeze(1))\n",
        "    return inputs_embeds, token_ids_tensor_one_hot\n",
        "\n",
        "def predict_and_visualize_transformer(input_text, model=transformer,\n",
        "                                      src_tokenizer=en_tokenizer,\n",
        "                                      tgt_tokenizer=fr_tokenizer,\n",
        "                                      src_vocab=en_vocab,\n",
        "                                      tgt_vocab=fr_vocab,\n",
        "                                      max_length=MAX_SENTENCE_LENGTH,\n",
        "                                      device=device):\n",
        "    input_text = normalizeString(input_text)\n",
        "    input_tokens = ['<bos>'] + src_tokenizer(input_text) + ['<eos>']\n",
        "    input_ids = [src_vocab[token] for token in input_tokens]\n",
        "    input_tensor = torch.tensor(input_ids).to(device).unsqueeze(1).to(device)\n",
        "\n",
        "    grads = []\n",
        "    i = 0\n",
        "    gen_tensor = torch.tensor([BOS_IDX]).unsqueeze(1).to(device)\n",
        "    while i < max_length:\n",
        "        # get embeddings and predict\n",
        "        src_key_padding_mask = (input_tensor == PAD_IDX).transpose(0, 1).to(device)\n",
        "        tgt_key_padding_mask = (gen_tensor == PAD_IDX).transpose(0, 1).to(device)\n",
        "\n",
        "        src_embed, token_tensor_one_hot = get_embeds(model.embed_src.weight, model.pos_enc, input_tensor, device)\n",
        "        tgt_embed, _ = get_embeds(model.embed_tgt.weight, model.pos_enc, gen_tensor, device)\n",
        "        logits = model(src_embeds=src_embed.to(device),\n",
        "                       tgt_embeds=tgt_embed.to(device),\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_mask = model.transformer.generate_square_subsequent_mask(gen_tensor.size(0)).to(device),\n",
        "                       tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "\n",
        "        # extract next-word logits\n",
        "        next_logits = logits[-1].squeeze()\n",
        "        pred_id = next_logits.argmax()\n",
        "        pred_logit = next_logits.max()\n",
        "\n",
        "        # get gradient-based saliency for this token wrt input sequence\n",
        "        saliency = ecco.attribution.compute_saliency_scores(pred_logit,\n",
        "                                                            token_tensor_one_hot,\n",
        "                                                            src_embed)\n",
        "        grads.append(saliency['gradient'])\n",
        "\n",
        "        # update generated sequence with new token\n",
        "        gen_tensor = torch.cat([gen_tensor, pred_id.unsqueeze(0).unsqueeze(1)]).to(device)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if pred_id == EOS_IDX:\n",
        "            break\n",
        "        if i == max_length:\n",
        "            break\n",
        "\n",
        "    gen_tokens = [tgt_vocab.lookup_token(i) for i in gen_tensor.squeeze()]\n",
        "    grad_array = np.stack(grads)\n",
        "\n",
        "    # plot it\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111)\n",
        "    m = ax.matshow(grad_array, cmap='gray')\n",
        "    plt.xticks(range(grad_array.shape[1]), input_tokens, rotation=45, fontsize=12)\n",
        "    plt.yticks(range(grad_array.shape[0]), gen_tokens[1:], fontsize=12)\n",
        "    plt.colorbar(m)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cftQWJKbfE_i"
      },
      "outputs": [],
      "source": [
        "predict_and_visualize_transformer(\"I am going to the store.\", device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFD4JLS6fjbe"
      },
      "outputs": [],
      "source": [
        "predict_and_visualize_transformer(\"How long will you be here?\", device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy1bOhyGfmaq"
      },
      "outputs": [],
      "source": [
        "predict_and_visualize_transformer(\"Can we please go to the library?\", device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qhURMfwfo1J"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_visualize_transformer(\"Her family moved away last year.\", device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iA4vvtHfrpX"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_visualize_transformer(\"Two thousand people fit into this hall.\", device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js7ZE5fBftq6"
      },
      "outputs": [],
      "source": [
        "if not FILTER_TO_BASIC_PREFIXES:\n",
        "  predict_and_visualize_transformer(\"The poor young man finally became a great artist.\", device='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM6T3cs1rCzp"
      },
      "source": [
        "## Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhRsmj-prEYc"
      },
      "outputs": [],
      "source": [
        "test_loss = {}\n",
        "test_loss['Bahdanau RNN'] = evaluate(seq2seq, test_iter, loss_fn, device)\n",
        "test_loss['Transformer'] = evaluate_transformer(transformer, test_iter, loss_fn, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plkUjjvMrF4W"
      },
      "outputs": [],
      "source": [
        "test_ppl = {k:np.exp(v) for k, v in test_loss.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yxGvSEUrJlV"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.bar(test_loss.keys(), test_loss.values(), color='gray')\n",
        "ax0.set_title(\"Test Set Loss\")\n",
        "ax0.set_ylabel(\"Cross-Entropy Loss\")\n",
        "\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.bar(test_ppl.keys(), test_ppl.values(), color='gray')\n",
        "ax1.set_title(\"Test Set Perplexity\")\n",
        "ax1.set_ylabel(\"Perplexity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdV8bq61coEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpbqs72dcsOu"
      },
      "source": [
        "### Old Visuals\n",
        "\n",
        "Keeping old code just in case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6yoQzWhfYgq"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(metrics, title, chart_filter=''):\n",
        "    data = pd.DataFrame(metrics).T\n",
        "    data = data[list(filter(re.compile('.*'+chart_filter + '.*').match,\n",
        "                         list(data.columns.values)))]\n",
        "    data.plot(figsize=(10,6), title=title).legend(bbox_to_anchor=(1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VRNlzbofa0R"
      },
      "outputs": [],
      "source": [
        "plot_metrics(bahdanau_metrics, \"Bahdanau Attention: Loss\", \"loss\")\n",
        "plot_metrics(bahdanau_metrics, \"Bahdanau Attention: Perplexity\", \"ppl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEC3Ld_ipvS_"
      },
      "outputs": [],
      "source": [
        "plot_metrics(transformer_metrics, \"Transformer: Loss\", \"loss\")\n",
        "plot_metrics(transformer_metrics, \"Transformer: Perplexity\", \"ppl\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Machine Translation Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}